{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Approach 2: Azure Cognitive Search Vector Search + Azure OpenAI \n",
    "This code demonstrates how to use Azure Cognitive Search by using the Push API to insert vectors into your search index.\n",
    "Adapted from: https://github.com/Azure/cognitive-search-vector-pr/blob/main/demo-python/code/azure-search-vector-python-sample.ipynb\n",
    "## Prerequisites\n",
    "To run the code, install the following packages. Please use the latest pre-release version `pip install azure-search-documents --pre`. This sample currently uses version `11.4.0b11`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from dotenv import load_dotenv  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient, SearchIndexingBufferedSender  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryLanguage,\n",
    "    QueryType,\n",
    "    RawVectorQuery,\n",
    "    VectorizableTextQuery,\n",
    "    VectorFilterMode,    \n",
    ")\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    ExhaustiveKnnVectorSearchAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    HnswParameters,  \n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmConfiguration,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    "    VectorSearchVectorizer,\n",
    "    VectorSearchVectorizerKind,\n",
    "    AzureOpenAIParameters,\n",
    "    AzureOpenAIVectorizer,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    ExhaustiveKnnVectorSearchAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings, \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    HnswParameters,  \n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    # VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIParameters,\n",
    "    AzureOpenAIVectorizer,\n",
    ")  \n",
    "\n",
    "# Additional Imports\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import Callable, Optional, Union\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "\n",
    "\n",
    "load_dotenv('./my.env')\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_base= os.getenv('OPENAI_API_BASE')\n",
    "openai.api_type= \"azure\"\n",
    "openai.api_version = os.getenv('OPENAI_API_VERSION')\n",
    "\n",
    "\n",
    "#overwriting to azure open_ai environment variables\n",
    "config = dotenv_values(\"./my.env\")\n",
    "openai.api_base = config[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "openai.api_version = config[\"AZURE_OPENAI_API_VERSION\"]\n",
    "credential = AzureKeyCredential(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ada_embedding(text: Union[str, list[str]]) -> list[list[float]]:\n",
    "    \"\"\"Generates embeddings using text-embedding-ada-002 model\n",
    "\n",
    "    Args:\n",
    "        text (Union[str, list[str]]): Text to generate embedding for\n",
    "\n",
    "    Returns:\n",
    "        list[list[float]]: List of embeddings for text item\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    embeddings = []\n",
    "    EMBEDDING_MODEL = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\",\"text-embedding-ada-002\")\n",
    "    # Azure OpenAI embedding API has max batch size of 16\n",
    "    step_size = 16\n",
    "    for n in range(step_size, len(text) + step_size, step_size):\n",
    "        res = openai.Embedding.create(\n",
    "            input=text[n - step_size : n], deployment_id='text-embedding-ada-002'\n",
    "        )\n",
    "        embeddings += [d.embedding for d in res.data]\n",
    "        time.sleep(5)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text(text, chunk_size: int, chunk_overlap: int, length_function: Callable[[str], int] = len):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = length_function\n",
    "    )\n",
    "    split_text = text_splitter.create_documents([text])\n",
    "    \n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "Read your data, generate OpenAI embeddings and export to a format to insert your Azure Cognitive Search index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_html = 'test_doc.html'\n",
    "doc_pdf = 'test_doc.pdf'\n",
    "\n",
    "\n",
    "input_data = {'pdf': {}, 'html':{}}\n",
    "loader = UnstructuredHTMLLoader(doc_html)\n",
    "data = loader.load()\n",
    "input_data['html']['title'] = 'html'\n",
    "input_data['html']['content'] = data[0].page_content\n",
    "\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(doc_pdf)\n",
    "data = loader.load()\n",
    "pages = len(data)\n",
    "pdf_content = ''\n",
    "\n",
    "for x in range(pages):\n",
    "    pdf_content = pdf_content + data[x].page_content\n",
    "\n",
    "input_data['pdf']['title'] = 'pdf'\n",
    "\n",
    "input_data['pdf']['content'] = pdf_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "chunk_overlap=0\n",
    "content_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#html\n",
    "content = input_data['html']['content']\n",
    "title = input_data['html']['title']\n",
    "split_text_html = split_text(content, chunk_size, chunk_overlap)\n",
    "\n",
    "\n",
    "for id_, chunk in tqdm(enumerate(split_text_html)):\n",
    "    chunk = chunk.page_content\n",
    "    data_json = {}\n",
    "    data_json['id'] = str(id_)\n",
    "    data_json['title'] = title\n",
    "    data_json['content'] = chunk\n",
    "    content_embeddings = generate_ada_embedding(chunk)[0]\n",
    "    data_json['contentVector'] = content_embeddings\n",
    "    content_list.append(data_json) # adding chunk to content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf\n",
    "content = input_data['pdf']['content']\n",
    "title = input_data['pdf']['title']\n",
    "split_text_pdf = split_text(content, chunk_size, chunk_overlap)\n",
    "\n",
    "\n",
    "for id_, chunk in tqdm(enumerate(split_text_pdf)):\n",
    "    chunk = chunk.page_content\n",
    "    data_json = {}\n",
    "    data_json['id'] = str(id_+len(split_text_html))\n",
    "    data_json['title'] = title\n",
    "    data_json['content'] = chunk\n",
    "    content_embeddings = generate_ada_embedding(chunk)[0]\n",
    "    data_json['contentVector'] = content_embeddings\n",
    "    content_list.append(data_json) # adding chunk to content_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output embeddings to docVectors.json file\n",
    "with open(\"./docVectors.json\", \"w\") as f:\n",
    "    json.dump(content_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'text-embedding-ada-002'\n",
    "index_name = 'mysearch3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your search index\n",
    "Create your search index schema and vector search configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile=\"myHnswProfile\"),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswVectorSearchAlgorithmConfiguration(\n",
    "            name=\"myHnsw\",\n",
    "            kind=VectorSearchAlgorithmKind.HNSW,\n",
    "            parameters=HnswParameters(\n",
    "                m=4,\n",
    "                ef_construction=400,\n",
    "                ef_search=500,\n",
    "                metric=\"cosine\"\n",
    "            )\n",
    "        ),\n",
    "        ExhaustiveKnnVectorSearchAlgorithmConfiguration(\n",
    "            name=\"myExhaustiveKnn\",\n",
    "            kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "            parameters=ExhaustiveKnnParameters(\n",
    "                metric=\"cosine\"\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm=\"myHnsw\",\n",
    "            vectorizer=\"myOpenAI\"\n",
    "        ),\n",
    "        VectorSearchProfile(\n",
    "            name=\"myExhaustiveKnnProfile\",\n",
    "            algorithm=\"myExhaustiveKnn\",\n",
    "            vectorizer=\"myOpenAI\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            name=\"myOpenAI\",\n",
    "            kind=\"azureOpenAI\",\n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(\n",
    "                resource_uri=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                deployment_id=model,\n",
    "                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "            )\n",
    "    )  \n",
    "]  \n",
    "\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into vector store\n",
    "Add texts and metadata from the JSON data to the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload some documents to the index\n",
    "with open('./docVectors.json', 'r') as file:  \n",
    "    chunks = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "print(f\"Uploaded {len(chunks)} chunks\") \n",
    "result = search_client.upload_documents(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are indexing a very large number of chunks, you can use the `SearchIndexingBufferedSender` which is an optimized way to automatically index the docs as it will handle the batching for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use SearchIndexingBufferedSender to upload the documents in batches optimized for indexing  \n",
    "with SearchIndexingBufferedSender(  \n",
    "    endpoint=service_endpoint,  \n",
    "    index_name=index_name,  \n",
    "    credential=credential,  \n",
    ") as batch_client:  \n",
    "    # Add upload actions for all documents  \n",
    "    batch_client.upload_documents(documents=chunks)  \n",
    "print(f\"Uploaded {len(chunks)} chunks in total\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results, string_out=150):\n",
    "      \n",
    "    for result in results:  \n",
    "        print(f\"Title: {result['title']}\")  \n",
    "        print(f\"Score: {result['@search.score']}\")  \n",
    "        print(f\"Content: {result['content'][0: string_out]}\\n\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a pure vector search using the vectorizable text query, all you need to do is pass in text and your vectorizer will handle the query vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "query = \"Am I entitled to Night Pay?\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k=3, fields=\"contentVector\")\n",
    "\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    ")  \n",
    "  \n",
    "print_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a pure vector search using a raw vector query, in this example, you are responsible for generating the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "query = \"Am I entitled to Night Pay?\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "vector_query = RawVectorQuery(vector=generate_ada_embedding(query)[0], k=3, fields=\"contentVector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    ")  \n",
    "  \n",
    "print_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a pure vector search to demonstrate OpenAI's text-embedding-ada-002 multilingual capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search multi-lingual (e.g '\"Am I entitled to Night Pay?\"  ' in French)  \n",
    "query = \"Suis-je éligible à une prime de nuit ?\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k=3, fields=\"contentVector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    ")  \n",
    "  \n",
    "print_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform an Exhaustive KNN exact nearest neighbor search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how you can exhaustively search your vector index regardless of what index you have, HNSW or ExhaustiveKNN. You can use this to calculate the ground-truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "query = \"Am I entitled to Night Pay?\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k=3, fields=\"contentVector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    ")  \n",
    "  \n",
    "print_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Multi-Vector Search\n",
    "\n",
    "This example shows a cross-field vector search that allows you to query multiple vector fields at the same time by passing in multiple query vectors. Note, in this case, you can pass in query vectors from two different embedding models to the corresponding vector fields in your index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Vector Search\n",
    "query = \"Am I entitled to Night Pay?\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)  \n",
    "vector_query_2 = VectorizableTextQuery(text=query, k=3, fields=\"contentVector\")  \n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries=[vector_query_2, vector_query_2],\n",
    "    select=[\"title\", \"content\"],\n",
    ")  \n",
    "  \n",
    "print_results(results)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Pure Vector Search with a filter\n",
    "This example shows how to apply filters on your index. Note, that you can choose whether you want to use Pre-Filtering (default) or Post-Filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "query = \"Am I entitled to Night Pay?\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "  \n",
    "vector_query = VectorizableTextQuery(text=query, k=3, fields=\"contentVector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    vector_filter_mode=VectorFilterMode.PRE_FILTER,\n",
    "    filter=\"title eq 'pdf'\",\n",
    "    select=[\"title\", \"content\"],\n",
    ")\n",
    "  \n",
    "print_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "query = \"Am I entitled to Night Pay?\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))  \n",
    "vector_query = VectorizableTextQuery(text=query, k=3, fields=\"contentVector\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    "    top=3\n",
    ")  \n",
    "\n",
    "print_results(results) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-svr",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
