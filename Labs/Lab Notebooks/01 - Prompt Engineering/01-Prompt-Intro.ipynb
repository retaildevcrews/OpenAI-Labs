{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompts\n",
    "\n",
    "\n",
    "*__NOTE:__\n",
    "For instructions on running the Jupyter Notebook that contains the labs see instructions here: <https://github.com/retaildevcrews/OpenAI-Labs>*\n",
    "\n",
    "Prompt engineering is the developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics (https://www.promptingguide.ai). This notebook provides a basic introduction to creating prompts for ChatGPT language model. A prompt has the following elements:\n",
    "- Instruction\n",
    "- Context\n",
    "- Input Data\n",
    "- Output Indicator\n",
    "\n",
    "Not all elements need to be present in a prompt.\n",
    "\n",
    "## Settings \n",
    "\n",
    "**Temperature** - controls randomness (or creativity), value between 0 and 1, it does this by affecting the probability distribution over the possible tokens at the generation step - a setting of 0 is deterministic, the higher the number the more \"creative\" the response.\n",
    "\n",
    "**Top_p** - Reduces the set of considered tokens to the top percentage, also value between 0 and 1, a setting of 0.1 will consider only the top 10% of the probability mass for the next token.\n",
    "\n",
    "*If you are looking for the model to return one right answer, lower number will be desireable. Higher numbers provide a more creative (unpredictable answer)*\n",
    "\n",
    "More Info - (https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/completions#prompt-design) and (https://www.promptingguide.ai/introduction/settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os module & the OpenAI Python library for calling the OpenAI API\n",
    "# please make sure you have installed required libraries via pip install -r requirements.txt\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config values\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "    \n",
    "# This is set to `azure`\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Setting up the deployment name\n",
    "chatgpt_model_name = os.getenv(\"CHATGPT_MODEL\")\n",
    "\n",
    "# Currently Chat Completions API have the following versions available: 2023-03-15-preview\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Define Request to OpenAI API\n",
    "For these labs we are using the Azure OpenAI Completion API, this is different than the Chat completion API.  Information about its usage can be found here:\n",
    "(https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#completions)\n",
    "The function defined below takes prompt and settings parameters sends the request to the endpoint and prints the text element from the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_prompt(prompt, temperature=0.5, top_p=0.5):\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"you are an assistant that performs completions\"},\n",
    "    {\"role\": \"user\", \"content\":prompt}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=chatgpt_model_name,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    response_content=response.choices[0].message.content\n",
    "    print(\"----\")\n",
    "    print(\"Settings:\")\n",
    "    print(\"temperature: \"+ str(temperature) + \" top_p: \" + str(top_p))\n",
    "    print(\"Prompt: \" + prompt)\n",
    "    print(\"Response: \")\n",
    "    print(str(response_content))\n",
    "    print(\"----\")\n",
    "    return response_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Simple Prompt - No instruction\n",
    "Example of a very simple prompt so we can see what the model returns for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\n",
    "The sky is\n",
    "\"\"\"\n",
    "send_prompt(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Run with different settings\n",
    "\n",
    "In these examples we will run the single prompt using different settings for Temperature and Top_p, the first number parameter sets temperature and second sets top_p.  Feel free to play with different values or add new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\n",
    "The sky is\n",
    "\"\"\"\n",
    "send_prompt(prompt,0,0)\n",
    "\n",
    "send_prompt(prompt,0.9,0.5)\n",
    "\n",
    "send_prompt(prompt,0.5,0.5)\n",
    "\n",
    "send_prompt(prompt,0,1)\n",
    "\n",
    "send_prompt(prompt,1,0)\n",
    "\n",
    "send_prompt(prompt,1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Prompt with Instructions\n",
    "These examples provide instructions in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\n",
    "Finish the following sentence and print the entire resulting sentence:\n",
    "The sky is\n",
    "\"\"\"\n",
    "send_prompt(prompt)\n",
    "\n",
    "\n",
    "prompt =  \"\"\"\n",
    "Translate the text below to Spanish:\n",
    "Text: \"hello!\"\n",
    "\"\"\"\n",
    "send_prompt(prompt)\n",
    "\n",
    "\n",
    "prompt =  \"\"\"\n",
    "Explain lacrosse\n",
    "\"\"\"\n",
    "send_prompt(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Prompt with Context\n",
    "These examples provide instructions in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\n",
    "Question: What was OKT3 originally sourced from?\n",
    "\"\"\"\n",
    "send_prompt(prompt)\n",
    "\n",
    "\n",
    "prompt =  \"\"\"\n",
    "Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
    "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
    "Question: What was OKT3 originally sourced from?\n",
    "\"\"\"\n",
    "send_prompt(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc180f703c9255d3d630e6d09ed4eb3355d27845db546035ce1b410f2bfa43b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
